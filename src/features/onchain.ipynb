{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ab3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/features/onchain.py\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Dict\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "HOUR = \"1H\"\n",
    "\n",
    "def _read_parquet(path: Optional[Path], cols: Optional[Sequence[str]] = None) -> Optional[pd.DataFrame]:\n",
    "    if path is None:\n",
    "        return None\n",
    "    df = pd.read_parquet(path)\n",
    "    if cols:\n",
    "        missing = set(cols) - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"{path} missing columns: {missing}\")\n",
    "        df = df[list(cols)]\n",
    "    return df\n",
    "\n",
    "def _ensure_ts_index(df: pd.DataFrame, ts_col: str, freq: str = HOUR) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[ts_col] = pd.to_datetime(out[ts_col], utc=True)\n",
    "    out = out.set_index(ts_col).sort_index()\n",
    "    return out\n",
    "\n",
    "def _resample_sum(df: pd.DataFrame, freq: str = HOUR) -> pd.DataFrame:\n",
    "    return df.resample(freq).sum(min_count=1)\n",
    "\n",
    "def _resample_mean(df: pd.DataFrame, freq: str = HOUR) -> pd.DataFrame:\n",
    "    return df.resample(freq).mean()\n",
    "\n",
    "def _resample_median(df: pd.DataFrame, freq: str = HOUR) -> pd.DataFrame:\n",
    "    return df.resample(freq).median()\n",
    "\n",
    "def _resample_quantile(df: pd.DataFrame, q: float, freq: str = HOUR) -> pd.DataFrame:\n",
    "    return df.resample(freq).quantile(q)\n",
    "\n",
    "def _zscore_rolling(x: pd.Series, window: str = \"168H\", min_periods: int = 24) -> pd.Series:\n",
    "    # 7d = 168 hours\n",
    "    mean = x.rolling(window, min_periods=min_periods).mean()\n",
    "    std = x.rolling(window, min_periods=min_periods).std(ddof=0)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def _load_labels(path: Optional[Path]) -> Dict[str, Dict]:\n",
    "    if path is None:\n",
    "        return {}\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _is_cex(addr: str, labels: Dict[str, Dict]) -> bool:\n",
    "    meta = labels.get(addr.lower())\n",
    "    return bool(meta and meta.get(\"is_exchange\", False))\n",
    "\n",
    "def _align_index(df: pd.DataFrame, idx: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    return df.reindex(idx)\n",
    "\n",
    "def build_onchain_mv_feature_set(\n",
    "    ts_start: str | pd.Timestamp,\n",
    "    ts_end: str | pd.Timestamp,\n",
    "    freq: str = HOUR,\n",
    "    # inputs\n",
    "    transfers_path: Optional[Path] = None,\n",
    "    labels_exchanges_path: Optional[Path] = None,\n",
    "    uniswap_swaps_path: Optional[Path] = None,\n",
    "    eth_blocks_path: Optional[Path] = None,\n",
    "    address_first_seen_path: Optional[Path] = None,\n",
    "    stable_tokens: Sequence[str] = (\"USDT\", \"USDC\", \"DAI\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns an hourly DataFrame with the 6 minimum-viable on-chain features (plus a few helpful extras).\n",
    "    Missing inputs are skipped gracefully (columns filled with NaN, index still returned).\n",
    "    \"\"\"\n",
    "    ts_start = pd.to_datetime(ts_start, utc=True)\n",
    "    ts_end = pd.to_datetime(ts_end, utc=True)\n",
    "    idx = pd.date_range(ts_start.floor(freq), ts_end.ceil(freq), freq=freq, tz=\"UTC\", inclusive=\"left\")\n",
    "    out = pd.DataFrame(index=idx)\n",
    "\n",
    "    # ==== Labels (exchanges) ====\n",
    "    labels = _load_labels(labels_exchanges_path)\n",
    "\n",
    "    # ==== 1) Exchange netflows: BTC/ETH native + 2) Stablecoin netflows ====\n",
    "    if transfers_path is not None:\n",
    "        tcols = [\"ts\", \"chain\", \"token\", \"from_address\", \"to_address\", \"amount\", \"amount_usd\"]\n",
    "        tr = _read_parquet(transfers_path, tcols)\n",
    "        tr = _ensure_ts_index(tr, \"ts\")\n",
    "        tr = tr.loc[ts_start:ts_end]\n",
    "\n",
    "        # Flag CEX direction\n",
    "        tr[\"from_is_cex\"] = tr[\"from_address\"].str.lower().map(lambda a: _is_cex(a, labels))\n",
    "        tr[\"to_is_cex\"] = tr[\"to_address\"].str.lower().map(lambda a: _is_cex(a, labels))\n",
    "\n",
    "        # BTC/ETH native netflows (in - out)\n",
    "        # Convention: rows for native should have token == \"BTC\" or \"ETH\" with amount in native units.\n",
    "        def _netflow(token_sym: str) -> pd.Series:\n",
    "            df = tr.query(\"token == @token_sym\")\n",
    "            inflow = df.loc[df[\"to_is_cex\"], [\"amount\"]].rename(columns={\"amount\": \"inflow\"})\n",
    "            outflow = df.loc[df[\"from_is_cex\"], [\"amount\"]].rename(columns={\"amount\": \"outflow\"})\n",
    "            inflow = _resample_sum(inflow, freq).rename(columns={\"inflow\": \"inflow\"})\n",
    "            outflow = _resample_sum(outflow, freq).rename(columns={\"outflow\": \"outflow\"})\n",
    "            nf = inflow.join(outflow, how=\"outer\")\n",
    "            return (nf[\"inflow\"].fillna(0) - nf[\"outflow\"].fillna(0)).rename(f\"ex_netflow_{token_sym.lower()}\")\n",
    "\n",
    "        try:\n",
    "            out[\"ex_netflow_btc\"] = _align_index(_netflow(\"BTC\").to_frame(), idx)[\"ex_netflow_btc\"]\n",
    "        except Exception:\n",
    "            out[\"ex_netflow_btc\"] = np.nan\n",
    "        try:\n",
    "            out[\"ex_netflow_eth\"] = _align_index(_netflow(\"ETH\").to_frame(), idx)[\"ex_netflow_eth\"]\n",
    "        except Exception:\n",
    "            out[\"ex_netflow_eth\"] = np.nan\n",
    "\n",
    "        # Stablecoin USD netflows (sum of tokens)\n",
    "        st = tr[tr[\"token\"].isin(stable_tokens)]\n",
    "        st_in = st.loc[st[\"to_is_cex\"], [\"amount_usd\"]]\n",
    "        st_out = st.loc[st[\"from_is_cex\"], [\"amount_usd\"]]\n",
    "        st_in = _resample_sum(st_in, freq).rename(columns={\"amount_usd\": \"in_usd\"})\n",
    "        st_out = _resample_sum(st_out, freq).rename(columns={\"amount_usd\": \"out_usd\"})\n",
    "        st_nf = st_in.join(st_out, how=\"outer\")\n",
    "        out[\"stables_netflow_cex_usd\"] = _align_index(\n",
    "            (st_nf[\"in_usd\"].fillna(0) - st_nf[\"out_usd\"].fillna(0)).to_frame(), idx\n",
    "        )[\"in_usd\"].fillna(0) - _align_index(st_nf[\"out_usd\"].to_frame(), idx)[\"out_usd\"].fillna(0)\n",
    "\n",
    "        # 5) Whale bursts (≥ $1M touching CEX)\n",
    "        whale = st[(st[\"amount_usd\"] >= 1_000_000) & (st[[\"from_is_cex\", \"to_is_cex\"]].any(axis=1))]\n",
    "        whale_cnt = _resample_sum(whale.assign(one=1)[[\"one\"]], freq).rename(columns={\"one\": \"whale_cex_tx_count_gt_1m\"})\n",
    "        whale_usd = _resample_sum(whale[[\"amount_usd\"]], freq).rename(columns={\"amount_usd\": \"whale_cex_tx_usd_gt_1m\"})\n",
    "        out = out.join(_align_index(whale_cnt, idx)).join(_align_index(whale_usd, idx))\n",
    "\n",
    "        # 6) Address activity breadth\n",
    "        # Unique active addresses per hour (senders U receivers)\n",
    "        active = (\n",
    "            tr.assign(hour=lambda d: d.index.floor(freq))\n",
    "              .groupby(\"hour\")\n",
    "              .apply(lambda g: pd.Series({\n",
    "                  \"active_addresses_hourly\": pd.unique(pd.concat([g[\"from_address\"], g[\"to_address\"]])).size\n",
    "              }))\n",
    "              .sort_index()\n",
    "        )\n",
    "        out = out.join(_align_index(active, idx))\n",
    "\n",
    "        # New addresses per hour (needs first_seen). If not provided, approximate by first occurrence in window.\n",
    "        if address_first_seen_path is not None:\n",
    "            fs = _read_parquet(address_first_seen_path, [\"address\", \"first_ts\"])\n",
    "            fs[\"first_ts\"] = pd.to_datetime(fs[\"first_ts\"], utc=True).dt.floor(freq)\n",
    "            new_counts = fs[(fs[\"first_ts\"] >= idx[0]) & (fs[\"first_ts\"] < idx[-1] + pd.Timedelta(freq))] \\\n",
    "                .groupby(\"first_ts\").size().rename(\"new_addresses_hourly\").to_frame()\n",
    "            new_counts.index.name = None\n",
    "            out = out.join(_align_index(new_counts, idx))\n",
    "        else:\n",
    "            # Approximate: first time seen within the requested window only\n",
    "            seen = set()\n",
    "            new_per_hour = []\n",
    "            for t in idx:\n",
    "                g = tr.loc[t:t + pd.Timedelta(freq) - pd.Timedelta(\"1ns\"), [\"from_address\", \"to_address\"]]\n",
    "                addrs = pd.unique(pd.concat([g[\"from_address\"], g[\"to_address\"]])).tolist()\n",
    "                new_now = sum(1 for a in addrs if a not in seen)\n",
    "                seen.update(addrs)\n",
    "                new_per_hour.append(new_now)\n",
    "            out[\"new_addresses_hourly\"] = new_per_hour\n",
    "\n",
    "    else:\n",
    "        # No transfers → fill feature columns with NaN\n",
    "        out[\"ex_netflow_btc\"] = np.nan\n",
    "        out[\"ex_netflow_eth\"] = np.nan\n",
    "        out[\"stables_netflow_cex_usd\"] = np.nan\n",
    "        out[\"whale_cex_tx_count_gt_1m\"] = np.nan\n",
    "        out[\"whale_cex_tx_usd_gt_1m\"] = np.nan\n",
    "        out[\"active_addresses_hourly\"] = np.nan\n",
    "        out[\"new_addresses_hourly\"] = np.nan\n",
    "\n",
    "    # ==== 3) DEX swap intensity & price impact ====\n",
    "    if uniswap_swaps_path is not None:\n",
    "        scol = [\"ts\", \"amount_usd\", \"mid_before\", \"mid_after\"]\n",
    "        sw = _read_parquet(uniswap_swaps_path, scol)\n",
    "        sw = _ensure_ts_index(sw, \"ts\")\n",
    "        sw = sw.loc[ts_start:ts_end]\n",
    "        sw[\"impact_bps\"] = (sw[\"mid_after\"] - sw[\"mid_before\"]).abs() / sw[\"mid_before\"].replace(0, np.nan) * 1e4\n",
    "        notional = _resample_sum(sw[[\"amount_usd\"]], freq).rename(columns={\"amount_usd\": \"dex_swap_notional_usd\"})\n",
    "        impact = _resample_mean(sw[[\"impact_bps\"]], freq).rename(columns={\"impact_bps\": \"dex_price_impact_bps\"})\n",
    "        out = out.join(_align_index(notional, idx)).join(_align_index(impact, idx))\n",
    "    else:\n",
    "        out[\"dex_swap_notional_usd\"] = np.nan\n",
    "        out[\"dex_price_impact_bps\"] = np.nan\n",
    "\n",
    "    # ==== 4) Gas/fee pressure ====\n",
    "    if eth_blocks_path is not None:\n",
    "        bcol = [\"ts_block\", \"basefee_gwei\", \"priority_fee_gwei\", \"gas_used\", \"gas_limit\"]\n",
    "        bl = _read_parquet(eth_blocks_path, bcol)\n",
    "        bl = _ensure_ts_index(bl, \"ts_block\")\n",
    "        bl = bl.loc[ts_start:ts_end]\n",
    "        bl[\"util\"] = bl[\"gas_used\"] / bl[\"gas_limit\"].replace(0, np.nan)\n",
    "        basefee = _resample_median(bl[[\"basefee_gwei\"]], freq)\n",
    "        tip_p90 = _resample_quantile(bl[[\"priority_fee_gwei\"]], 0.90, freq).rename(columns={\"priority_fee_gwei\": \"gas_tip_p90_gwei\"})\n",
    "        pct_full = (\n",
    "            bl.assign(full=lambda d: (d[\"util\"] >= 0.95).astype(int))[[\"full\"]]\n",
    "              .resample(freq).mean().rename(columns={\"full\": \"pct_blocks_full\"})\n",
    "        )\n",
    "        out = out.join(_align_index(basefee, idx)).join(_align_index(tip_p90, idx)).join(_align_index(pct_full, idx))\n",
    "    else:\n",
    "        out[\"basefee_gwei\"] = np.nan\n",
    "        out[\"gas_tip_p90_gwei\"] = np.nan\n",
    "        out[\"pct_blocks_full\"] = np.nan\n",
    "\n",
    "    # ==== Z-scores (7-day) for a few key pressure series ====\n",
    "    for col in [\"ex_netflow_btc\", \"ex_netflow_eth\", \"stables_netflow_cex_usd\",\n",
    "                \"dex_swap_notional_usd\", \"dex_price_impact_bps\",\n",
    "                \"whale_cex_tx_count_gt_1m\", \"active_addresses_hourly\",\n",
    "                \"basefee_gwei\", \"pct_blocks_full\"]:\n",
    "        if col in out.columns:\n",
    "            out[f\"{col}_z7d\"] = _zscore_rolling(out[col])\n",
    "\n",
    "    return out.sort_index()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "jupytext,text_representation,kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
