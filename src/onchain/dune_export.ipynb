{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse, os, sys, time, pathlib, json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import requests\n",
    "\n",
    "\n",
    "def to_iso(s: Optional[str]) -> Optional[str]:\n",
    "    return s if s else None\n",
    "\n",
    "\n",
    "def run_dune_saved_query(api_key: str, query_id: int, params: Dict[str, Any]) -> pd.DataFrame:\n",
    "    base = \"https://api.dune.com/api/v1\"\n",
    "    headers = {\"X-DUNE-API-KEY\": api_key, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # execute\n",
    "    r = requests.post(f\"{base}/query/{query_id}/execute\", headers=headers, json={\"query_parameters\": params or {}}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    exe_id = j.get(\"execution_id\")\n",
    "    if not exe_id:\n",
    "        raise RuntimeError(f\"Execute failed: {j}\")\n",
    "\n",
    "    # poll with 429 backoff\n",
    "    delay, max_delay = 1.0, 10.0\n",
    "    for _ in range(240):\n",
    "        sr = requests.get(f\"{base}/execution/{exe_id}/status\", headers=headers, timeout=30)\n",
    "        if sr.status_code == 429:\n",
    "            time.sleep(delay); delay = min(delay*1.6, max_delay); continue\n",
    "        sr.raise_for_status()\n",
    "        st = sr.json()\n",
    "        state = st.get(\"state\") or st.get(\"execution_state\")\n",
    "        if state in {\"COMPLETED\",\"QUERY_STATE_COMPLETED\"}: break\n",
    "        if state in {\"FAILED\",\"CANCELLED\",\"QUERY_STATE_FAILED\"}:\n",
    "            raise RuntimeError(f\"Query failed: {st}\")\n",
    "        time.sleep(min(delay, max_delay))\n",
    "    else:\n",
    "        raise TimeoutError(\"Timed out waiting for Dune execution to complete.\")\n",
    "\n",
    "    # fetch with 429 backoff\n",
    "    delay = 1.0\n",
    "    while True:\n",
    "        rr = requests.get(f\"{base}/execution/{exe_id}/results\", headers=headers, timeout=60)\n",
    "        if rr.status_code == 429:\n",
    "            time.sleep(delay); delay = min(delay*1.6, max_delay); continue\n",
    "        rr.raise_for_status()\n",
    "        break\n",
    "\n",
    "    res = rr.json()\n",
    "    rows = (res.get(\"result\") or {}).get(\"rows\") or res.get(\"rows\") or []\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def load_jobs_yaml(path: str) -> List[Dict[str, Any]]:\n",
    "    data = yaml.safe_load(open(path, \"r\"))\n",
    "    jobs = data.get(\"jobs\") if isinstance(data, dict) else data\n",
    "    if not isinstance(jobs, list):\n",
    "        raise ValueError(\"Invalid jobs YAML: expected top-level 'jobs:' list.\")\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def ensure_outdir(d: str) -> None:\n",
    "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_parquet(df: pd.DataFrame, out_path: str) -> None:\n",
    "    df.to_parquet(out_path, index=False)\n",
    "\n",
    "\n",
    "def parse_iso(s: str) -> datetime:\n",
    "    # naive 'Z' handling\n",
    "    if s.endswith('Z'): s = s[:-1]\n",
    "    return datetime.fromisoformat(s.replace('Z','')).replace(tzinfo=timezone.utc)\n",
    "\n",
    "\n",
    "def daterange(start: datetime, end: datetime, step: timedelta):\n",
    "    t = start\n",
    "    while t < end:\n",
    "        n = min(t + step, end)\n",
    "        yield t, n\n",
    "        t = n\n",
    "\n",
    "\n",
    "def run_job(api_key: str, job: Dict[str, Any], start: Optional[str], end: Optional[str], sleep: float) -> Dict[str, Any]:\n",
    "    name = job.get(\"name\")\n",
    "    qid  = job.get(\"query_id\")\n",
    "    out  = job.get(\"outfile\")\n",
    "    chunk_hours = float(job.get(\"chunk_hours\", 0))  # 0 = no chunking\n",
    "    chunk_minutes = float(job.get(\"chunk_minutes\", 0))  # 0 = no chunking\n",
    "\n",
    "    if not (name and qid and out):\n",
    "        return {\"job\": name, \"error\": \"Job missing name/query_id/outfile\"}\n",
    "\n",
    "    out_path = str(pathlib.Path(out)) if out.startswith(\"data/\") else str(pathlib.Path(\"data/processed/onchain\")/out)\n",
    "    print(f\"→ {name}: query_id={qid} → {out_path}\", file=sys.stderr)\n",
    "\n",
    "    # if no chunking requested or no start/end provided, do a single call\n",
    "    if (not chunk_hours and not chunk_minutes) or not (start and end):\n",
    "        df = run_dune_saved_query(api_key, int(qid), {\"start\": to_iso(start), \"end\": to_iso(end)})\n",
    "        write_parquet(df, out_path)\n",
    "        return {\"job\": name, \"rows\": int(len(df)), \"outfile\": out_path}\n",
    "\n",
    "    # chunked pulls\n",
    "    start_dt, end_dt = parse_iso(start), parse_iso(end)\n",
    "    step = (timedelta(minutes=chunk_minutes) if chunk_minutes else timedelta(hours=chunk_hours))\n",
    "    parts = []\n",
    "    for i, (s_dt, e_dt) in enumerate(daterange(start_dt, end_dt, step), 1):\n",
    "        s_iso = s_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        e_iso = e_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        try:\n",
    "            dfp = run_dune_saved_query(api_key, int(qid), {\"start\": s_iso, \"end\": e_iso})\n",
    "            if not dfp.empty:\n",
    "                parts.append(dfp)\n",
    "            print(f\"   chunk {i}: {s_iso} → {e_iso} rows={len(dfp)}\", file=sys.stderr)\n",
    "            if sleep: time.sleep(sleep)\n",
    "        except requests.HTTPError as he:\n",
    "            # If 402 on a chunk, tell the user and stop (can reduce chunk_hours further)\n",
    "            return {\"job\": name, \"error\": f\"HTTPError on chunk {s_iso}..{e_iso}: {he}\"}\n",
    "        except Exception as e:\n",
    "            return {\"job\": name, \"error\": f\"Chunk {s_iso}..{e_iso} failed: {e}\"}\n",
    "\n",
    "    df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    write_parquet(df, out_path)\n",
    "    return {\"job\": name, \"rows\": int(len(df)), \"outfile\": out_path}\n",
    "\n",
    "\n",
    "def main() -> int:\n",
    "    ap = argparse.ArgumentParser(description=\"Dune saved-query exporter (free-tier friendly, with chunking).\")\n",
    "    ap.add_argument(\"--jobs\", required=True)\n",
    "    ap.add_argument(\"--outdir\", required=True)\n",
    "    ap.add_argument(\"--job\")\n",
    "    ap.add_argument(\"--start\")\n",
    "    ap.add_argument(\"--end\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.0, help=\"Sleep seconds between calls (and chunks).\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    api_key = os.getenv(\"DUNE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"ERROR: DUNE_API_KEY not set.\", file=sys.stderr)\n",
    "        return 2\n",
    "\n",
    "    jobs = load_jobs_yaml(args.jobs)\n",
    "    ensure_outdir(args.outdir)\n",
    "\n",
    "    summary = []\n",
    "    for job in jobs:\n",
    "        if args.job and job.get(\"name\") != args.job:\n",
    "            continue\n",
    "        # run and write using outdir prefix\n",
    "        outfile = job.get(\"outfile\"); \n",
    "        job[\"outfile\"] = str(pathlib.Path(args.outdir)/outfile)\n",
    "        res = run_job(api_key, job, args.start, args.end, args.sleep)\n",
    "        summary.append(res)\n",
    "\n",
    "    ok = all(\"error\" not in s for s in summary if s.get(\"job\"))\n",
    "    print(json.dumps({\"ok\": ok, \"summary\": summary}, indent=2))\n",
    "    return 0 if ok else 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raise SystemExit(main())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,text_representation,kernelspec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
