{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cb6e72",
   "metadata": {},
   "source": [
    "# Data Quality Checks (OHLCV)\n",
    "- Schema summary (types, null %, unique counts)\n",
    "- Missingness & duplicates (rows, timestamps)\n",
    "- Time gaps vs. expected interval\n",
    "- OHLCV sanity (high/low bounds, negatives)\n",
    "- Return outliers (z-scores)\n",
    "- Saves a JSON report + CSVs + figures to reports/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f885a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Plot defaults (project-wide)\n",
    "This cell ensures consistent Matplotlib styling and date axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 'configs' importable from notebooks (.ipynb or .py)\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "try:\n",
    "    HERE = Path(__file__).parent\n",
    "except NameError:\n",
    "    HERE = Path.cwd()\n",
    "ROOT = (HERE / \"../../..\").resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "from configs.plots.mpl_defaults import use_mpl_defaults, format_date_axis\n",
    "use_mpl_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = ROOT / \"data\" / \"processed\"\n",
    "REPORT_FIGS = ROOT / \"reports\" / \"figures\"\n",
    "REPORT_TBLS = ROOT / \"reports\" / \"tables\"\n",
    "REPORT_FIGS.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_TBLS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b01490",
   "metadata": {},
   "source": [
    "## 1) Load data (prefers `data/raw`, falls back to `data/processed`)\n",
    "Supports CSV (expects `timestamp`) and Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50568bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = [f for f in os.listdir(DATA_RAW) if f.lower().endswith((\".csv\",\".parquet\"))] if DATA_RAW.exists() else []\n",
    "src = DATA_RAW if cands else DATA_PROCESSED\n",
    "files = [f for f in os.listdir(src) if f.lower().endswith((\".csv\",\".parquet\"))]\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No CSV/Parquet found in data/raw or data/processed.\")\n",
    "path = (src / sorted(files)[0])\n",
    "if str(path).endswith(\".csv\"):\n",
    "    df = pd.read_csv(path, parse_dates=[\"timestamp\"], infer_datetime_format=True)\n",
    "else:\n",
    "    df = pd.read_parquet(path)\n",
    "if \"timestamp\" not in df.columns:\n",
    "    raise ValueError(\"Expected a 'timestamp' column.\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=False)\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "print(\"Loaded:\", path, \"shape:\", df.shape)\n",
    "\n",
    "# Try to coerce common OHLCV columns to numeric\n",
    "for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db71a1",
   "metadata": {},
   "source": [
    "## 2) Schema summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pd.DataFrame({\n",
    "    \"dtype\": df.dtypes.astype(str),\n",
    "    \"non_null\": df.notna().sum(),\n",
    "    \"nulls\": df.isna().sum(),\n",
    "})\n",
    "schema[\"null_pct\"] = (schema[\"nulls\"] / len(df) * 100).round(3)\n",
    "schema[\"nunique\"] = df.nunique(dropna=True)\n",
    "schema.reset_index(names=\"column\", inplace=True)\n",
    "schema_path = REPORT_TBLS / \"dq_schema.csv\"\n",
    "schema.to_csv(schema_path, index=False)\n",
    "schema.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b341ab",
   "metadata": {},
   "source": [
    "## 3) Duplicates & timestamp health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c152f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_rows = int(df.duplicated().sum())\n",
    "dup_ts = int(df.duplicated(subset=[\"timestamp\"]).sum())\n",
    "print(\"Duplicate rows:\", dup_rows, \" Duplicate timestamps:\", dup_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f213c",
   "metadata": {},
   "source": [
    "## 4) Time gaps vs expected sampling\n",
    "Estimate median interval; list the top gaps and approximate missing bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c47a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsec = df[\"timestamp\"].diff().dt.total_seconds()\n",
    "median_sec = float(dsec.dropna().median())\n",
    "expected = max(1.0, median_sec)  # seconds\n",
    "gaps = dsec[dsec > expected * 1.5].sort_values(ascending=False)\n",
    "gap_tbl = pd.DataFrame({\n",
    "    \"timestamp\": df.loc[gaps.index, \"timestamp\"].astype(str),\n",
    "    \"gap_seconds\": gaps.astype(float),\n",
    "    \"missing_intervals_est\": ((gaps / expected) - 1).round(2)\n",
    "})\n",
    "gap_csv = REPORT_TBLS / \"dq_time_gaps.csv\"\n",
    "gap_tbl.to_csv(gap_csv, index=False)\n",
    "gap_tbl.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25365620",
   "metadata": {},
   "source": [
    "## 5) OHLCV sanity checks\n",
    "- No negatives for prices; volume ≥ 0\n",
    "- high ≥ max(open,close,low) and low ≤ min(open,close,low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = {}\n",
    "if set([\"open\",\"high\",\"low\",\"close\"]).issubset(df.columns):\n",
    "    o,h,l,c = df[\"open\"], df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    issues[\"neg_price_rows\"] = int(((o<0)|(h<0)|(l<0)|(c<0)).sum())\n",
    "    issues[\"high_bound_viol\"] = int((h < pd.concat([o,l,c], axis=1).max(axis=1)).sum())\n",
    "    issues[\"low_bound_viol\"]  = int((l > pd.concat([o,l,c], axis=1).min(axis=1)).sum())\n",
    "if \"volume\" in df.columns:\n",
    "    issues[\"neg_volume_rows\"] = int((df[\"volume\"] < 0).sum())\n",
    "issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda2dc3",
   "metadata": {},
   "source": [
    "## 6) Return outliers (z-scores on log returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"logret\"] = np.log(df[\"close\"]).diff() if \"close\" in df.columns else np.nan\n",
    "mu, sd = df[\"logret\"].mean(), df[\"logret\"].std()\n",
    "z = (df[\"logret\"] - mu) / (sd if sd and sd>0 else 1.0)\n",
    "out_idx = z.abs().sort_values(ascending=False).head(20).index\n",
    "outliers = df.loc[out_idx, [\"timestamp\",\"close\",\"logret\"]].assign(z=z.loc[out_idx].values).sort_values(\"timestamp\")\n",
    "out_csv = REPORT_TBLS / \"dq_return_outliers.csv\"\n",
    "outliers.to_csv(out_csv, index=False)\n",
    "outliers.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeaefde",
   "metadata": {},
   "source": [
    "## 7) Missingness bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isna().sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(null_counts.index[:40], null_counts.values[:40])\n",
    "plt.xticks(rotation=75, ha=\"right\")\n",
    "plt.title(\"Null counts by column (top 40)\")\n",
    "plt.tight_layout()\n",
    "fig_nulls = REPORT_FIGS / \"dq_null_counts.png\"\n",
    "plt.savefig(fig_nulls)\n",
    "print(\"Saved:\", fig_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cd1a1",
   "metadata": {},
   "source": [
    "## 8) Report summary (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d300154",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    \"dataset\": os.path.basename(path),\n",
    "    \"rows\": int(len(df)),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"time_start\": df[\"timestamp\"].min().isoformat() if len(df) else None,\n",
    "    \"time_end\": df[\"timestamp\"].max().isoformat() if len(df) else None,\n",
    "    \"median_step_seconds\": median_sec,\n",
    "    \"duplicate_rows\": dup_rows,\n",
    "    \"duplicate_timestamps\": dup_ts,\n",
    "    \"gap_csv\": str(gap_csv),\n",
    "    \"schema_csv\": str(schema_path),\n",
    "    \"outliers_csv\": str(out_csv),\n",
    "    \"nulls_fig\": str(fig_nulls),\n",
    "    \"ohlcv_issues\": issues\n",
    "}\n",
    "rep_json = REPORT_TBLS / \"dq_report.json\"\n",
    "with open(rep_json, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"Saved report:\", rep_json)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,text_representation,kernelspec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
