{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f8d18",
   "metadata": {},
   "source": [
    "# Leakage Stress Tests\n",
    "Tools to detect potential target leakage and time-split issues.\n",
    "- ACF of target (autocorrelation)\n",
    "- Lead/lag correlation scan: features vs. shifted target (future info?)\n",
    "- Random split vs. time-based split comparison (red flag if random >> time)\n",
    "- Simple equality/duplication checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a98e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../..\"))\n",
    "DATA_PROCESSED = os.path.join(ROOT, \"data\", \"processed\")\n",
    "REPORT_FIGS = os.path.join(ROOT, \"reports\", \"figures\")\n",
    "os.makedirs(REPORT_FIGS, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe07e11",
   "metadata": {},
   "source": [
    "## 1) Load a processed dataset\n",
    "Picks the **first** parquet under `data/processed/`. Adjust path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1756ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqs = [f for f in os.listdir(DATA_PROCESSED) if f.endswith(\".parquet\")]\n",
    "if not parqs:\n",
    "    raise FileNotFoundError(\"No parquet in data/processed. Run features pipeline first.\")\n",
    "path = os.path.join(DATA_PROCESSED, sorted(parqs)[0])\n",
    "df = pd.read_parquet(path)\n",
    "print(\"Loaded:\", path, \"shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba5a20",
   "metadata": {},
   "source": [
    "## 2) Identify time, target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time index\n",
    "time_col = None\n",
    "for c in [\"timestamp\", \"date\", \"time\"]:\n",
    "    if c in df.columns:\n",
    "        time_col = c\n",
    "        break\n",
    "if time_col is None:\n",
    "    raise ValueError(\"No time column found. Add 'timestamp' or 'date'.\")\n",
    "\n",
    "df[time_col] = pd.to_datetime(df[time_col], utc=False)\n",
    "df = df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "# Target column heuristics\n",
    "target_candidates = [c for c in df.columns if re.search(r\"^(y|target|vol_?target|ret_h\\\\d+)$\", c)]\n",
    "target_col = target_candidates[0] if target_candidates else None\n",
    "\n",
    "# Fallback: proxy target via forward realized vol of a return column\n",
    "if target_col is None:\n",
    "    ret_cols = [c for c in df.columns if c.startswith(\"ret_\")]\n",
    "    if not ret_cols:\n",
    "        raise ValueError(\"No explicit target or return columns to build proxy.\")\n",
    "    base = ret_cols[0]\n",
    "    horizon = 12\n",
    "    df[\"y_proxy\"] = df[base].rolling(horizon).apply(lambda x: np.sqrt(np.sum(np.square(x))), raw=True)\n",
    "    target_col = \"y_proxy\"\n",
    "    print(\"Using proxy target:\", target_col)\n",
    "\n",
    "# Feature set\n",
    "exclude = {time_col, target_col}\n",
    "meta_like = {c for c in df.columns if c.lower() in {\"symbol\",\"asset\"}}\n",
    "features = [c for c in df.columns if c not in exclude.union(meta_like)]\n",
    "\n",
    "print(\"time_col:\", time_col)\n",
    "print(\"target_col:\", target_col)\n",
    "print(\"n_features:\", len(features))\n",
    "\n",
    "# Keep numeric features only\n",
    "num_features = df[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "X_full = df[num_features]\n",
    "y_full = df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceaeea5",
   "metadata": {},
   "source": [
    "## 3) Target autocorrelation (ACF)\n",
    "High autocorrelation is common in vol series; useful context for leakage tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9eb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 60\n",
    "acf_vals = []\n",
    "for L in range(lags + 1):\n",
    "    acf_vals.append(y_full.autocorr(lag=L))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.stem(range(lags + 1), acf_vals, use_line_collection=True)\n",
    "plt.title(f\"Target ACF (0..{lags})\")\n",
    "plt.xlabel(\"Lag\"); plt.ylabel(\"Autocorr\")\n",
    "plt.tight_layout()\n",
    "acf_png = os.path.join(REPORT_FIGS, \"leakage_target_acf.png\")\n",
    "plt.savefig(acf_png)\n",
    "print(\"Saved ACF plot:\", acf_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6d354",
   "metadata": {},
   "source": [
    "## 4) Lead/Lag correlation scan\n",
    "If a feature is more correlated with a **future** target (negative shift), it may contain lookahead info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_lags = [-10, -5, -1, 0, 1, 5, 10]\n",
    "lead_lag_report = {}\n",
    "y = y_full.copy()\n",
    "\n",
    "for f in num_features:\n",
    "    s = df[f]\n",
    "    corr_by_shift = {}\n",
    "    for k in lead_lags:\n",
    "        # Shift target by k: negative k => compare feature with future target\n",
    "        corr = s.corr(y.shift(k))\n",
    "        corr_by_shift[int(k)] = None if pd.isna(corr) else float(corr)\n",
    "    # Best (absolute) correlation and where it occurs\n",
    "    best_shift = max(corr_by_shift, key=lambda kk: abs(corr_by_shift[kk]) if corr_by_shift[kk] is not None else -1)\n",
    "    lead_lag_report[f] = {\n",
    "        \"corr_by_shift\": corr_by_shift,\n",
    "        \"best_shift\": int(best_shift),\n",
    "        \"best_corr\": corr_by_shift[best_shift]\n",
    "    }\n",
    "\n",
    "# Flag suspicious: best shift is negative (aligns with **future** target)\n",
    "suspicious_features = [f for f, v in lead_lag_report.items() if v[\"best_shift\"] < 0 and v[\"best_corr\"] is not None and abs(v[\"best_corr\"]) > 0.1]\n",
    "print(\"Suspicious (future-aligned) features:\", suspicious_features[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bcf67",
   "metadata": {},
   "source": [
    "## 5) Equality/duplicate checks (hard leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_equal = [f for f in num_features if df[f].equals(y_full)]\n",
    "dupe_pairs = []\n",
    "seen = {}\n",
    "for c in num_features:\n",
    "    sig = (df[c].astype(\"float64\").fillna(-1234567.89).values.tobytes())\n",
    "    if sig in seen:\n",
    "        dupe_pairs.append((c, seen[sig]))\n",
    "    else:\n",
    "        seen[sig] = c\n",
    "\n",
    "print(\"Exact equals to target:\", exact_equal)\n",
    "print(\"Duplicate feature pairs (first 5):\", dupe_pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffcafd",
   "metadata": {},
   "source": [
    "## 6) Random split vs. time split (red-flag gap)\n",
    "If performance on a random split is much higher than on a proper time split, leakage or temporal dependency issues are likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20924804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare matrices\n",
    "X_num = X_full.fillna(0.0).values\n",
    "y_num = y_full.values\n",
    "\n",
    "# Chronological split (last 20% for validation)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_tr, X_va = X_num[:split_idx], X_num[split_idx:]\n",
    "y_tr, y_va = y_num[:split_idx], y_num[split_idx:]\n",
    "\n",
    "rf_time = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42, n_jobs=-1)\n",
    "rf_time.fit(X_tr, y_tr)\n",
    "pred_time = rf_time.predict(X_va)\n",
    "rmse_time = mean_squared_error(y_va, pred_time, squared=False)\n",
    "r2_time = r2_score(y_va, pred_time)\n",
    "\n",
    "# Random split (same size) for comparison\n",
    "rng = np.random.default_rng(42)\n",
    "perm = rng.permutation(len(df))\n",
    "cut = int(len(df) * 0.8)\n",
    "idx_tr, idx_va = perm[:cut], perm[cut:]\n",
    "rf_rand = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42, n_jobs=-1)\n",
    "rf_rand.fit(X_num[idx_tr], y_num[idx_tr])\n",
    "pred_rand = rf_rand.predict(X_num[idx_va])\n",
    "rmse_rand = mean_squared_error(y_num[idx_va], pred_rand, squared=False)\n",
    "r2_rand = r2_score(y_num[idx_va], pred_rand)\n",
    "\n",
    "print(f\"Time-split  RMSE={rmse_time:.4f} R2={r2_time:.3f}\")\n",
    "print(f\"Random-split RMSE={rmse_rand:.4f} R2={r2_rand:.3f}\")\n",
    "\n",
    "gap_r2 = float(r2_rand - r2_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de459d",
   "metadata": {},
   "source": [
    "## 7) Save report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d448ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    \"dataset\": os.path.basename(path),\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"time_start\": df[time_col].min().isoformat(),\n",
    "    \"time_end\": df[time_col].max().isoformat(),\n",
    "    \"target\": target_col,\n",
    "    \"acf_top_lags\": {str(i): float(acf_vals[i]) for i in range(min(10, len(acf_vals)))},\n",
    "    \"suspicious_future_aligned_features\": suspicious_features[:50],\n",
    "    \"lead_lag_scan\": {f: v for f, v in list(lead_lag_report.items())[:200]},  # cap size\n",
    "    \"exact_equals_to_target\": exact_equal,\n",
    "    \"duplicate_feature_pairs\": dupe_pairs[:50],\n",
    "    \"time_split\": {\"rmse\": float(rmse_time), \"r2\": float(r2_time)},\n",
    "    \"random_split\": {\"rmse\": float(rmse_rand), \"r2\": float(r2_rand)},\n",
    "    \"r2_gap_random_minus_time\": gap_r2\n",
    "}\n",
    "out_json = os.path.join(REPORT_FIGS, \"leakage_checks_report.json\")\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"Saved report:\", out_json)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,text_representation,kernelspec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
