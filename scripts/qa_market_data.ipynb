{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b967f1d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import argparse, json, math\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39f620",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def pct(n, d): return float(n)/d*100 if d else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f363f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_parquet(path, ts_col=None):\n",
    "    df = pd.read_parquet(path)\n",
    "    if ts_col and ts_col in df.columns:\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col], utc=True)\n",
    "        df = df.set_index(ts_col).sort_index()\n",
    "    elif isinstance(df.index, pd.DatetimeIndex):\n",
    "        if df.index.tz is None:\n",
    "            df.index = df.index.tz_localize(\"UTC\")\n",
    "        else:\n",
    "            df.index = df.index.tz_convert(\"UTC\")\n",
    "        df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209d04c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ohlcv_checks(df):\n",
    "    probs = []\n",
    "    cols = set(df.columns)\n",
    "    needed = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "    missing = sorted(list(needed - cols))\n",
    "    if missing: probs.append(f\"Missing OHLCV columns: {missing}\")\n",
    "\n",
    "    if {\"open\",\"high\",\"low\",\"close\"} <= cols:\n",
    "        bad_high = (df[\"high\"] < df[[\"open\",\"close\",\"low\"]].max(axis=1)).sum()\n",
    "        bad_low  = (df[\"low\"]  > df[[\"open\",\"close\",\"high\"]].min(axis=1)).sum()\n",
    "        if bad_high: probs.append(f\"{bad_high} bars have high < max(open,close,low)\")\n",
    "        if bad_low:  probs.append(f\"{bad_low} bars have low  > min(open,close,high)\")\n",
    "        neg_px = (df[[\"open\",\"high\",\"low\",\"close\"]] <= 0).sum().sum()\n",
    "        if neg_px: probs.append(f\"{neg_px} non-positive price fields\")\n",
    "\n",
    "    if \"volume\" in cols:\n",
    "        neg_v = (df[\"volume\"] < 0).sum()\n",
    "        if neg_v: probs.append(f\"{neg_v} negative volume rows\")\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ba4fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def gap_dup_checks(idx, expected_freq=None):\n",
    "    out = {}\n",
    "    out[\"tz_aware\"] = idx.tz is not None\n",
    "    out[\"start\"] = str(idx.min()) if len(idx) else None\n",
    "    out[\"end\"] = str(idx.max()) if len(idx) else None\n",
    "    out[\"rows\"] = int(len(idx))\n",
    "    out[\"dupes\"] = int(idx.duplicated().sum())\n",
    "    if expected_freq:\n",
    "        full = pd.date_range(idx.min(), idx.max(), freq=expected_freq, tz=\"UTC\") if len(idx) else pd.DatetimeIndex([], tz=\"UTC\")\n",
    "        out[\"expected_bars\"] = int(len(full))\n",
    "        out[\"missing_bars\"] = int(len(full.difference(idx)))\n",
    "        out[\"missing_pct\"] = pct(out[\"missing_bars\"], out[\"expected_bars\"]) if out[\"expected_bars\"] else 0.0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833b693",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def jump_checks(close, label):\n",
    "    if close.isnull().all():\n",
    "        return { \"label\": label, \"note\": \"close all NA\" }\n",
    "    ret = np.log(close).diff()\n",
    "    q = ret.quantile([0.01,0.05,0.5,0.95,0.99]).to_dict()\n",
    "    big = ret.abs() > 0.15  # >15% log-move in one bar\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"ret_count\": int(ret.notna().sum()),\n",
    "        \"ret_abs_gt15pct\": int(big.sum()),\n",
    "        \"quantiles\": {str(k): float(v) for k,v in q.items()}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820f634",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def basis_checks(spot_c, perp_c):\n",
    "    df = pd.concat({\"spot\": spot_c, \"perp\": perp_c}, axis=1).dropna()\n",
    "    if df.empty: return {\"rows\": 0}\n",
    "    basis = (df[\"perp\"]/df[\"spot\"] - 1.0)\n",
    "    out = {\n",
    "        \"rows\": int(len(df)),\n",
    "        \"basis_mean_bp\": float(basis.mean()*1e4),\n",
    "        \"basis_p5_bp\": float(basis.quantile(0.05)*1e4),\n",
    "        \"basis_p95_bp\": float(basis.quantile(0.95)*1e4),\n",
    "        \"basis_abs_gt_200bp\": int((basis.abs()>0.02).sum())\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd661a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def funding_checks(funding_df):\n",
    "    # Expect Binance perp funding every 8h; check spacing and range\n",
    "    out = {\"rows\": int(len(funding_df))}\n",
    "    if \"funding_rate\" in funding_df.columns:\n",
    "        fr = funding_df[\"funding_rate\"].dropna()\n",
    "        out[\"rate_min_bp\"] = float(fr.min()*1e4) if len(fr) else None\n",
    "        out[\"rate_max_bp\"] = float(fr.max()*1e4) if len(fr) else None\n",
    "        # spacing\n",
    "        if isinstance(funding_df.index, pd.DatetimeIndex) and len(funding_df)>1:\n",
    "            dt = funding_df.index.to_series().diff().dropna().dt.total_seconds()/3600\n",
    "            out[\"spacing_hours_top3\"] = list(map(float, dt.value_counts().head(3).index))\n",
    "            out[\"pct_spacing_8h\"] = float((dt.round()==8).mean()*100)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2bb44d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--spot\",   required=True, help=\"spot parquet path (OHLCV)\")\n",
    "    ap.add_argument(\"--perp\",   required=True, help=\"perp parquet path (OHLCV)\")\n",
    "    ap.add_argument(\"--funding\",required=True, help=\"Binance perp funding parquet\")\n",
    "    ap.add_argument(\"--timeframe\", default=\"5min\", help=\"expected bar freq for spot/perp (e.g., 5min, 1h)\")\n",
    "    ap.add_argument(\"--outdir\", default=\"runs/_qa\", help=\"output directory\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    spot = load_parquet(args.spot)\n",
    "    perp = load_parquet(args.perp)\n",
    "    fund = load_parquet(args.funding, ts_col=\"timestamp\" if \"timestamp\" in pd.read_parquet(args.funding).columns else None)\n",
    "\n",
    "    # Basic shape & gaps\n",
    "    spot_gap = gap_dup_checks(spot.index, expected_freq=args.timeframe)\n",
    "    perp_gap = gap_dup_checks(perp.index, expected_freq=args.timeframe)\n",
    "    fund_gap = gap_dup_checks(fund.index)\n",
    "\n",
    "    # OHLCV sanity\n",
    "    spot_prob = ohlcv_checks(spot)\n",
    "    perp_prob = ohlcv_checks(perp)\n",
    "\n",
    "    # Jump anomalies\n",
    "    spot_jump = jump_checks(spot.get(\"close\"), \"spot\")\n",
    "    perp_jump = jump_checks(perp.get(\"close\"), \"perp\")\n",
    "\n",
    "    # Spot vs Perp basis\n",
    "    basis = basis_checks(spot.get(\"close\"), perp.get(\"close\"))\n",
    "\n",
    "    # Funding\n",
    "    funding = funding_checks(fund)\n",
    "\n",
    "    # Compose report\n",
    "    summary = {\n",
    "        \"spot_path\": args.spot,\n",
    "        \"perp_path\": args.perp,\n",
    "        \"funding_path\": args.funding,\n",
    "        \"timeframe_expectation\": args.timeframe,\n",
    "        \"spot_gaps\": spot_gap,\n",
    "        \"perp_gaps\": perp_gap,\n",
    "        \"funding_gaps\": fund_gap,\n",
    "        \"spot_problems\": spot_prob,\n",
    "        \"perp_problems\": perp_prob,\n",
    "        \"spot_jumps\": spot_jump,\n",
    "        \"perp_jumps\": perp_jump,\n",
    "        \"basis\": basis,\n",
    "        \"funding\": funding,\n",
    "    }\n",
    "\n",
    "    # Write JSON + Markdown\n",
    "    (outdir / \"dq_report.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "    md = []\n",
    "    md.append(\"# Data Quality Report\\n\")\n",
    "    md.append(f\"**Spot:** `{args.spot}`  \\n**Perp:** `{args.perp}`  \\n**Funding:** `{args.funding}`  \\n\")\n",
    "    md.append(\"\\n## 1) Time index & gaps\\n\")\n",
    "    for name, g in [(\"Spot\", spot_gap), (\"Perp\", perp_gap), (\"Funding\", fund_gap)]:\n",
    "        line = f\"- **{name}**: rows={g.get('rows')} tz_aware={g.get('tz_aware')} start={g.get('start')} end={g.get('end')}\"\n",
    "        if \"expected_bars\" in g:\n",
    "            line += f\" expected={g['expected_bars']} missing={g['missing_bars']} ({g.get('missing_pct',0):.2f}%)\"\n",
    "        line += f\" dupes={g.get('dupes',0)}\"\n",
    "        md.append(line)\n",
    "    md.append(\"\\n## 2) OHLCV sanity\\n\")\n",
    "    md.append(f\"- Spot: {'; '.join(spot_prob) if spot_prob else 'OK'}\")\n",
    "    md.append(f\"- Perp: {'; '.join(perp_prob) if perp_prob else 'OK'}\")\n",
    "    md.append(\"\\n## 3) Return jumps (log)\\n\")\n",
    "    md.append(f\"- Spot: {spot_jump}\")\n",
    "    md.append(f\"- Perp: {perp_jump}\")\n",
    "    md.append(\"\\n## 4) Spot vs Perp basis\\n\")\n",
    "    md.append(f\"- {basis}\")\n",
    "    md.append(\"\\n## 5) Funding cadence & range\\n\")\n",
    "    md.append(f\"- {funding}\")\n",
    "    (outdir / \"dq_report.md\").write_text(\"\\n\".join(md))\n",
    "    print(f\"Wrote: {outdir/'dq_report.md'} and {outdir/'dq_report.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,text_representation,kernelspec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
